# 0700_3_CIFAR10_FineTuning_Augmentation

import tensorflow as tf
import datetime

# 1ï¸âƒ£ [ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬]
print("ğŸš€ CIFAR-10 ë°ì´í„° ë¡œë“œ ì¤‘...")
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

x_train = tf.keras.applications.mobilenet_v2.preprocess_input(x_train.astype('float32'))
x_test = tf.keras.applications.mobilenet_v2.preprocess_input(x_test.astype('float32'))

nb_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, nb_classes)
y_test = tf.keras.utils.to_categorical(y_test, nb_classes)

# 2ï¸âƒ£ [ì „ëµ B: ë°ì´í„° ì¦ê°• (Data Augmentation)]
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
], name="data_augmentation")

# 3ï¸âƒ£ [ì¼íƒ€ ê°•ì‚¬ ì˜ì… ë° ëª¨ë¸ ì¡°ë¦½]
print("ğŸ§  ì‚¬ì „ í•™ìŠµëœ MobileNetV2 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
base_model = tf.keras.applications.MobileNetV2(input_shape=(96, 96, 3), 
                                               include_top=False, 
                                               weights='imagenet')
# 1ë‹¨ê³„ í•™ìŠµì„ ìœ„í•´ ì¼ë‹¨ ê½ê½ ì–¼ë ¤ë‘¡ë‹ˆë‹¤.
base_model.trainable = False 

# ì´ë²ˆì—ëŠ” ì¡°ê¸ˆ ë” ì„¸ë°€í•œ ì»¨íŠ¸ë¡¤ì„ ìœ„í•´ í•¨ìˆ˜í˜• API(Functional API) ë°©ì‹ìœ¼ë¡œ ì¡°ë¦½í•©ë‹ˆë‹¤.
inputs = tf.keras.Input(shape=(32, 32, 3))
x = data_augmentation(inputs)
x = tf.keras.layers.UpSampling2D(size=(3, 3))(x)

# â­ ì¤‘ìš”: training=Falseë¡œ ì„¤ì •í•˜ì—¬ base_model ë‚´ë¶€ì˜ ë°°ì¹˜ ì •ê·œí™”(BatchNorm) ì¸µì´ 
# ë¯¸ì„¸ ì¡°ì •(Fine-Tuning) ì¤‘ì—ë„ ë§ê°€ì§€ì§€ ì•Šë„ë¡ ë³´í˜¸í•©ë‹ˆë‹¤.
x = base_model(x, training=False) 

x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dropout(0.5)(x)
outputs = tf.keras.layers.Dense(nb_classes, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

# [ìˆ˜ë™ ê·¸ë˜í”„ ì¶”ì  (Manual Graph Tracing)]
log_dir = "logs/finetuning/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
writer = tf.summary.create_file_writer(log_dir)
tf.summary.trace_on(graph=True, profiler=False)
_ = model(tf.zeros((1, 32, 32, 3))) 
with writer.as_default():
    tf.summary.trace_export(name="fine_tuning_graph", step=0)

tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False)

# =========================================================================
# ğŸ¯ [1ë‹¨ê³„ í•™ìŠµ: ì›Œë°ì—… (ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµ)]
# =========================================================================
print("\n" + "="*50)
print("ğŸš€ [1ë‹¨ê³„] ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ì›Œë°ì—… í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤ (Base Model Frozen)...")
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # ê¸°ë³¸ í•™ìŠµë¥ 
              metrics=['accuracy'])

history_phase1 = model.fit(x_train, y_train,
                           batch_size=128,
                           epochs=5, # ì›Œë°ì—…ì€ 5ë²ˆë§Œ ì§§ê²Œ ì§„í–‰
                           validation_data=(x_test, y_test),
                           callbacks=[tensorboard_cb])

# =========================================================================
# ğŸ¯ [2ë‹¨ê³„ í•™ìŠµ: ë¯¸ì„¸ ì¡°ì • (Fine-Tuning)]
# =========================================================================
print("\n" + "="*50)
print("ğŸ”¥ [2ë‹¨ê³„] ì „ëµ A ì ìš©: ë¯¸ì„¸ ì¡°ì •(Fine-Tuning)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

# ì¼íƒ€ ê°•ì‚¬ì˜ ë‡Œ(ì „ì²´ ì¸µ)ì˜ ì–¼ìŒì„ ëª¨ë‘ ë…¹ì…ë‹ˆë‹¤.
base_model.trainable = True

# í•˜ì§€ë§Œ ë„ˆë¬´ ê¸°ì´ˆì ì¸ ì§€ì‹(ì•ìª½ ì¸µ)ê¹Œì§€ ê±´ë“œë¦¬ë©´ ì—­íš¨ê³¼ê°€ ë‚˜ë¯€ë¡œ, 
# 100ë²ˆì§¸ ì¸µ ì´ì „ì€ ë‹¤ì‹œ ì–¼ë ¤ë‘ê³ , ê¹Šê³  ë³µì¡í•œ íŠ¹ì§•ì„ ì¡ëŠ” 100ë²ˆì§¸ ì¸µ ì´í›„ë§Œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
fine_tune_at = 100
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# â­ í•µì‹¬: ì´ë¯¸ ë˜‘ë˜‘í•œ ìƒíƒœì´ë¯€ë¡œ, ê¸°ì¡´ ì§€ì‹ì´ íŒŒê´´ë˜ì§€ ì•Šê²Œ ë³´í­(í•™ìŠµë¥ )ì„ 1/100 ìˆ˜ì¤€ìœ¼ë¡œ ì•„ì£¼ ì‘ê²Œ ì¤„ì…ë‹ˆë‹¤!
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), # 1e-5 (ë§¤ìš° ë‚®ìŒ)
              metrics=['accuracy'])
model.summary()

checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    filepath='best_finetuned_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1
)

# 1ë‹¨ê³„ì—ì„œ 5ë²ˆ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ, ì´ì–´ì„œ ì¶”ê°€ë¡œ 10ë²ˆ(ì´ 15 ì—í¬í¬) ë” í•™ìŠµì‹œí‚µë‹ˆë‹¤.
history_phase2 = model.fit(x_train, y_train,
                           batch_size=128,
                           epochs=15, 
                           initial_epoch=history_phase1.epoch[-1] + 1, # 5ë²ˆë¶€í„° ì´ì–´ì„œ ì‹œì‘
                           validation_data=(x_test, y_test),
                           callbacks=[tensorboard_cb, checkpoint_cb])

# =========================================================================
# 4ï¸âƒ£ [ìµœì¢… í‰ê°€]
print("\n" + "="*50)
evaluation = model.evaluate(x_test, y_test)
print(f"ìµœì¢… Loss: {evaluation[0]:.4f}")
print(f"ìµœì¢… ì‹¤ì „ ì •í™•ë„(Accuracy): {evaluation[1]*100:.2f}%")



#===========================================================================
# ì „ì´ í•™ìŠµ ë¯¸ì„¸ ì¡°ì • + ë°ì´í„° ì¦ê°• ê²°í•©
# 1ë‹¨ê³„: ìƒˆë¡œ ë¶™ì¸ ë°ì´í„° ì¦ê°• í•„í„°ì™€ ë¶„ë¥˜ê¸°ë§Œ ë¨¼ì € CIFAR-10ì— ì ì‘ì‹œí‚¨ë‹¤.
# 2ë‹¨ê³„: ë¶„ë¥˜ê¸°ê°€ ì–´ëŠ ì •ë„ ë˜‘ë˜‘í•´ì§€ë©´ ë‡Œì˜ ê¹Šì€ ì¸µì„ ì‚´ì§ ë…¹ì´ê³  ë‚®ì€ í•™ìŠµë¥ ë¡· í•™ìŠµì‹œí‚¨ë‹¤.

# ì—í¬í¬ë‹¹ 14~17ì´ˆ ì •ë„ì˜ ì†ë„ë¡œ í•™ìŠµì´ ì§„í–‰ë˜ì—ˆë‹¤.
# 5ë²ˆì¨° ì—í¬í¬ì—ì„œ ê²€ì¦ ì •í™•ë„ê°€ 72.54%ê¹Œì§€ ì˜¬ë¼ê°”ë‹¤.

# ë¯¸ì„¸ ì¡°ì •ì„ í†µí•´ì„œ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ 16ë§Œê°œì—ì„œ 202ë§Œê°œë¡œ 12ë°°ì´ìƒ ì¦ê°€í•˜ì˜€ë‹¤.
# ì—í¬í¬ë‹¹ ì†Œìš” ì‹œê°„ì´ 26ì—ì„œ 33ì´ˆë¡œ ëŠ˜ì–´ë‚¬ë‹¤.
# ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ 82.16%ë¡œ ì˜¬ëë‹¤.

# í•™ìŠµ íšŸìˆ˜ê°€ ë¶€ì¡±í•˜ë‹¤. ì¦ê°• í•„í„°ê°€ ì¼œì ¸ ìˆë“œë©´ ìµœì†Œ 30ì—ì„œ 50ë²ˆì˜ ì—í¬í¬ê°€ í•„ìš”í•˜ë‹¤.
# MobileNetV2ëŠ” ê²½ë ¹í™” ëª¨ë¸ì´ë¯€ë¡œ í•œê³„ê°€ ëª…í™•í•˜ë‹¤.
