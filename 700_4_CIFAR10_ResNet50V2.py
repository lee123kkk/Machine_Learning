# 700_4_CIFAR10_ResNet50V2_Ultimate

import tensorflow as tf
import datetime

# =========================================================================
# 1ï¸âƒ£ [ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬] - ResNet50V2 ì „ìš© ì „ì²˜ë¦¬
# =========================================================================
print("ğŸš€ CIFAR-10 ë°ì´í„° ë¡œë“œ ì¤‘...")
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# ResNet50V2 ëª¨ë¸ì´ ì¢‹ì•„í•˜ëŠ” ë°©ì‹(-1 ~ 1 ë²”ìœ„)ìœ¼ë¡œ í”½ì…€ ê°’ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.
x_train = tf.keras.applications.resnet_v2.preprocess_input(x_train.astype('float32'))
x_test = tf.keras.applications.resnet_v2.preprocess_input(x_test.astype('float32'))

nb_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, nb_classes)
y_test = tf.keras.utils.to_categorical(y_test, nb_classes)

# =========================================================================
# 2ï¸âƒ£ [ìŠ¤ë§ˆíŠ¸ ê¸°ë²• 1: ê°•í™”ëœ ë°ì´í„° ì¦ê°• (Advanced Augmentation)]
# =========================================================================
# ê¸°ì¡´ íšŒì „, ì¤Œì— ë”í•´ ìƒí•˜ì¢Œìš° ì´ë™(Translation)ê¹Œì§€ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ í˜¹ë…í•˜ê²Œ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤.
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.15),
    tf.keras.layers.RandomZoom(0.15),
    tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),
], name="data_augmentation")

# =========================================================================
# 3ï¸âƒ£ [ì „ëµ D & C: ì´ˆì¼íƒ€ ê°•ì‚¬ ì˜ì… ë° í•´ìƒë„ ëŒ€í­ í™•ëŒ€]
# =========================================================================
print("ğŸ§  ì´ˆì¼íƒ€ ê°•ì‚¬ 'ResNet50V2'ë¥¼ ëª¨ì…”ì˜µë‹ˆë‹¤ (íŒŒë¼ë¯¸í„° ì•½ 2,300ë§Œ ê°œ)...")
base_model = tf.keras.applications.ResNet50V2(input_shape=(160, 160, 3), # í•´ìƒë„ë¥¼ 160x160ìœ¼ë¡œ ê³ ì •
                                              include_top=False, 
                                              weights='imagenet')
base_model.trainable = False # 1ë‹¨ê³„ ì›Œë°ì—…ì„ ìœ„í•´ ì–¼ë ¤ë‘ 

inputs = tf.keras.Input(shape=(32, 32, 3))
x = data_augmentation(inputs)

# â­ ì „ëµ C: 32x32 ì´ë¯¸ì§€ë¥¼ ê°€ë¡œì„¸ë¡œ 5ë°°ì”© ëŠ˜ë ¤ 160x160ìœ¼ë¡œ ë»¥íŠ€ê¸°í•©ë‹ˆë‹¤! (UpSampling)
# ì´ë¯¸ì§€ì˜ ì—¬ë°±ì´ ë§ì•„ì ¸ì„œ ResNetì´ íŠ¹ì§•ì„ í›¨ì”¬ ì •ë°€í•˜ê²Œ ì¡ì•„ëƒ…ë‹ˆë‹¤.
x = tf.keras.layers.UpSampling2D(size=(5, 5))(x)

# ë¼ˆëŒ€ ëª¨ë¸ í†µê³¼ (ë°°ì¹˜ ì •ê·œí™” íŒŒê´´ ë°©ì§€ë¥¼ ìœ„í•´ training=False)
x = base_model(x, training=False)

# ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° ë¶€ì°© (ìš©ëŸ‰ì´ í° ëª¨ë¸ì´ë¯€ë¡œ ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜ë„ 256ê°œë¡œ ëŠ˜ë¦¼)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dropout(0.5)(x)
outputs = tf.keras.layers.Dense(nb_classes, activation='softmax')(x)

model = tf.keras.Model(inputs, outputs)

# =========================================================================
# 4ï¸âƒ£ [ìˆ˜ë™ ê·¸ë˜í”„ ì¶”ì  ë° í…ì„œë³´ë“œ ì„¤ì •]
# =========================================================================
log_dir = "logs/ultimate/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
writer = tf.summary.create_file_writer(log_dir)

print("\nğŸ“¹ í…ì„œë³´ë“œìš© ëª¨ë¸ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ë…¹í™”í•©ë‹ˆë‹¤...")
tf.summary.trace_on(graph=True, profiler=False)
_ = model(tf.zeros((1, 32, 32, 3))) # ê°€ì§œ ë°ì´í„°ë¡œ íë¦„ ìƒì„±
with writer.as_default():
    tf.summary.trace_export(name="ultimate_graph", step=0)

tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=False)

# =========================================================================
# ğŸ¯ [1ë‹¨ê³„ í•™ìŠµ: ë¶„ë¥˜ê¸° ì›Œë°ì—… (5 Epochs)]
# =========================================================================
print("\n" + "="*50)
print("ğŸš€ [1ë‹¨ê³„] ë¶„ë¥˜ê¸° ì›Œë°ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ë¹ ë¥´ê²Œ ì§„í–‰ë©ë‹ˆë‹¤)...")
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])

# RTX 4060ì˜ VRAM(8GB)ì„ ê³ ë ¤í•˜ì—¬ Batch Sizeë¥¼ 64ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ë©”ëª¨ë¦¬ ì´ˆê³¼ ì‹œ 32ë¡œ ì¤„ì´ì„¸ìš”)
BATCH_SIZE = 64 

history_phase1 = model.fit(x_train, y_train,
                           batch_size=BATCH_SIZE,
                           epochs=5,
                           validation_data=(x_test, y_test),
                           callbacks=[tensorboard_cb])

# =========================================================================
# ğŸ¯ [2ë‹¨ê³„ í•™ìŠµ: ë¯¸ì„¸ ì¡°ì • ë° ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©]
# =========================================================================
print("\n" + "="*50)
print("ğŸ”¥ [2ë‹¨ê³„] ì¼íƒ€ ê°•ì‚¬ì˜ ë´‰ì¸ì„ í•´ì œí•˜ê³  ê·¹í•œì˜ í›ˆë ¨ì— ëŒì…í•©ë‹ˆë‹¤...")

base_model.trainable = True
# ResNet50V2ì˜ ê¹Šì€ ì¸µ(ì•½ 100ë²ˆì§¸ ì¸µ ì´í›„)ë§Œ ë…¹ì—¬ì„œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
fine_tune_at = 100
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# =========================================================================
# ğŸ§  [ìŠ¤ë§ˆíŠ¸ ê¸°ë²• 2 & 3: ì½œë°±(Callbacks) ì„¤ì •]
# =========================================================================
# 1. ModelCheckpoint: ê°€ì¥ ìµœê³  ì ìˆ˜ë¥¼ ê¸°ë¡í•œ ìˆœê°„ì˜ ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    filepath='ultimate_resnet_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1
)

# 2. EarlyStopping: 10ë²ˆ ì—°ì† ê°±ì‹ ì´ ì—†ìœ¼ë©´ ë¬´ì˜ë¯¸í•œ í›ˆë ¨ìœ¼ë¡œ íŒë‹¨í•˜ê³  ìë™ ì¢…ë£Œ
early_stopping_cb = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1
)

# 3. â­ ReduceLROnPlateau (ë™ì  í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§): 
# í•™ìŠµì´ ì •ì²´ê¸°(Plateau)ì— ë¹ ì§€ë©´ ë³´í­(Learning Rate)ì„ ì ˆë°˜(0.5)ìœ¼ë¡œ í™• ì¤„ì—¬ì„œ 
# ë§ˆì¹˜ í˜„ë¯¸ê²½ìœ¼ë¡œ ë³´ë“¯ ë¯¸ì„¸í•˜ê²Œ íŠœë‹í•©ë‹ˆë‹¤. ì´ê²Œ 90% ëŒíŒŒì˜ í•µì‹¬ í‚¤ì…ë‹ˆë‹¤.
lr_scheduler_cb = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1
)

model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005), # Phase 1ë³´ë‹¤ í›¨ì”¬ ì‘ê²Œ ì‹œì‘
              metrics=['accuracy'])
model.summary()

# í›ˆë ¨ ì‹œì‘ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë‹ˆ ì»¤í”¼ í•œ ì” ë“œì‹œê³  ì˜¤ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤!)
print("\nì‹œê°„ì´ ê½¤ ì†Œìš”ë©ë‹ˆë‹¤. í…ì„œë³´ë“œë¥¼ ì¼œë‘ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ê·¸ë˜í”„ê°€ ì˜¤ë¥´ëŠ” ê²ƒì„ ê°ìƒí•˜ì„¸ìš”!")
history_phase2 = model.fit(x_train, y_train,
                           batch_size=BATCH_SIZE,
                           epochs=50, # ìµœëŒ€ 50ë²ˆ ì¶”ê°€ ì§„í–‰ (ì´ 55íšŒ)
                           initial_epoch=history_phase1.epoch[-1] + 1,
                           validation_data=(x_test, y_test),
                           callbacks=[tensorboard_cb, checkpoint_cb, early_stopping_cb, lr_scheduler_cb])

# =========================================================================
# 4ï¸âƒ£ [ìµœì¢… í‰ê°€]
# =========================================================================
print("\n" + "="*50)
evaluation = model.evaluate(x_test, y_test)
print(f"ğŸ‰ ìµœì¢… Loss: {evaluation[0]:.4f}")
print(f"ğŸ† ìµœì¢… ì‹¤ì „ ì •í™•ë„(Accuracy): {evaluation[1]*100:.2f}%")


