#MNIST_Noise_Robustness

import numpy as np
import random
import tensorflow as tf
import matplotlib.pyplot as plt # ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ë¥¼ ëˆˆìœ¼ë¡œ ë³´ê¸° ìœ„í•´ ì¶”ê°€

# 1. [ì‹œë“œ ê³ ì •]
random.seed(777)
tf.random.set_seed(777)

# 2. [í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •]
learning_rate = 0.001
batch_size = 100
training_epochs = 15
nb_classes = 10
drop_rate = 0.3 # ë“œë¡­ì•„ì›ƒì€ ë…¸ì´ì¦ˆ ì œê±°ì—ë„ í° ë„ì›€ì„ ì¤ë‹ˆë‹¤.

# 3. [ë°ì´í„° ë¡œë“œ]
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# 4. [ë°ì´í„° ì „ì²˜ë¦¬]
# (1) í‰íƒ„í™” ë° ì •ê·œí™”
x_train = x_train.reshape(x_train.shape[0], 28 * 28).astype('float32') / 255.0
x_test = x_test.reshape(x_test.shape[0], 28 * 28).astype('float32') / 255.0

# (2) ì›-í•« ì¸ì½”ë”©
y_train = tf.keras.utils.to_categorical(y_train, nb_classes)
y_test = tf.keras.utils.to_categorical(y_test, nb_classes)

# =================================================================
# [â­ í•µì‹¬ ì¶”ê°€] ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆ(ì¡ìŒ) ê°•ì œë¡œ ì£¼ì…í•˜ê¸°
# =================================================================
noise_factor = 0.5 # ë…¸ì´ì¦ˆ ê°•ë„ (0.0 ~ 1.0, í´ìˆ˜ë¡ ì§€ì €ë¶„í•´ì§)

print(f"ğŸŒ«ï¸ ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ì£¼ì…í•©ë‹ˆë‹¤ (ê°•ë„: {noise_factor})...")

# np.random.normal: ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë¬´ì‘ìœ„ ì¡ìŒì„ ìƒì„±í•´ ì›ë³¸ì— ë”í•¨
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

# np.clip: ë…¸ì´ì¦ˆë¥¼ ë”í•˜ë‹¤ ë³´ë©´ í”½ì…€ê°’ì´ 1.0ì„ ë„˜ê±°ë‚˜ 0.0ë³´ë‹¤ ì‘ì•„ì§ˆ ìˆ˜ ìˆìŒ.
# ì´ë¥¼ 0.0 ~ 1.0 ì‚¬ì´ë¡œ ê°•ì œë¡œ ì˜ë¼ëƒ„ (ìœ íš¨ ë²”ìœ„ ìœ ì§€)
x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)

# [ğŸ‘€ ì‹œê°í™”] ë…¸ì´ì¦ˆê°€ ë‚€ ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.
plt.figure(figsize=(10, 4))
for i in range(5):
    # ì›ë³¸ ì´ë¯¸ì§€
    ax = plt.subplot(2, 5, i + 1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title("Original")
    plt.axis('off')

    # ë…¸ì´ì¦ˆ ì´ë¯¸ì§€
    ax = plt.subplot(2, 5, i + 1 + 5)
    plt.imshow(x_train_noisy[i].reshape(28, 28), cmap='gray')
    plt.title("Noisy")
    plt.axis('off')
plt.show()
# =================================================================

# 5. [ëª¨ë¸ êµ¬ì„±] Lab 10-5ì™€ ë™ì¼ (Deep + Wide + Dropout)
tf.model = tf.keras.Sequential()
tf.model.add(tf.keras.layers.Dense(input_dim=784, units=512, kernel_initializer='glorot_normal', activation='relu'))
tf.model.add(tf.keras.layers.Dropout(drop_rate))
tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))
tf.model.add(tf.keras.layers.Dropout(drop_rate))
tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))
tf.model.add(tf.keras.layers.Dropout(drop_rate))
tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))
tf.model.add(tf.keras.layers.Dropout(drop_rate))
tf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer='glorot_normal', activation='softmax'))

# 6. [ì»´íŒŒì¼]
tf.model.compile(loss='categorical_crossentropy',
                 optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
                 metrics=['accuracy'])

# 7. [í•™ìŠµ ìˆ˜í–‰] *ì¤‘ìš”: x_train ëŒ€ì‹  x_train_noisyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤!
print("\nğŸ”¥ ë…¸ì´ì¦ˆê°€ ì„ì¸ ì´ë¯¸ì§€ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
history = tf.model.fit(x_train_noisy, y_train, batch_size=batch_size, epochs=training_epochs)

# 8. [ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸]
y_predicted = tf.model.predict(x_test_noisy)

print("\n" + "="*50)
print("ğŸ” ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸")
print("="*50)
for x in range(0, 10):
    random_index = random.randint(0, x_test_noisy.shape[0]-1)
    
    actual_val = np.argmax(y_test[random_index])
    pred_val = np.argmax(y_predicted[random_index])
    
    result = "âœ… ì •ë‹µ" if actual_val == pred_val else "âŒ ì˜¤ë‹µ"
    print(f"Index: {random_index:<5} | ì •ë‹µ: {actual_val} vs ì˜ˆì¸¡: {pred_val} -> {result}")

# 9. [ìµœì¢… í‰ê°€]
evaluation = tf.model.evaluate(x_test_noisy, y_test)
print("="*50)
print(f"ë…¸ì´ì¦ˆ í™˜ê²½ ìµœì¢… ì •í™•ë„: {evaluation[1]*100:.2f}%")
#===============================================================
# ë“œë¡­ì•„ì›ƒ ì ìš© ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ì”ëœ© ë‚€ ì•…ì¡°ê±´ ì†ì—ì„œ ìˆ«ìë¥¼ ë§ì¶°ë‚´ëŠ” AI
# í˜„ì‹¤ ì„¸ê³„ì—ì„œì˜ ë°ì´í„°ì²˜ëŸ¬ ì¡í‹°ê°€ ë§ì€ ë°ì´í„°ë¥¼ ì„ì–´ì„œ í•™ìŠµ

# ë…¸ì´ì¦ˆë¥¼ ë”í•˜ë©´ 0ì—ì„œ 1ì‚¬ì´ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ë“¤ì´ ìƒê¸°ëŠ”ë° ì´ ê°’ë“¤ì„ ì œê±°í•˜ëŠ” í´ë¦¬í•‘ì„ ìˆ˜í–‰í•œë‹¤.

# ì²«ë²ˆì§¸ ì—í¬í¬ëŠ” 74.55%ë¡œ ê¹¨ë—í•œ ë°ì´í„°ë¥¼ ì¼ì„ ë•Œ ë³´ë‹¤ ë‚®ê²Œ ì¶œë°œí–ˆë‹¤.
# ì—í¬í¬ê°€ ì§€ë‚ ìˆ˜ë¡ ì •í™•ë„ê°€ ìƒìŠ¹í•´ì„œ 96.64%ì— ë„ë‹¬í–ˆë‹¤.
# ìµœì¢… ì •í™•ë„ëŠ” 92.43%ë¡œ ë…¸ì´ì¦ˆê°€ ì—†ì„ ë•Œë³´ë‹¤ ë–¨ì–´ì¡Œì§€ë§Œ, ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒˆë‹¤.

# í•™ìŠµ ë°ì´í„°ì— ì¼ë¶€ëŸ¬ ë…¸ì´ì¦ˆë¥¼ ì„ì–´ í›ˆë ¨í•˜ë©´ ì•…ì¡°ê±´ ì†ì—ì„œë„ ì‘ë™í•˜ëŠ” AIë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
