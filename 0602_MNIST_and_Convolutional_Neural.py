# Lab 11 MNIST and Convolutional Neural Network

'''
ğŸ“š CNN(í•©ì„±ê³± ì‹ ê²½ë§)ì˜ êµ¬ì¡°ì™€ ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.

ğŸ§  ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì— CNNì„ ì–´ë–»ê²Œ ì ìš©í•˜ëŠ”ì§€ ì‹¤ìŠµí•©ë‹ˆë‹¤.

ğŸ§ª í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì˜ íš¨ê³¼ì™€ ì¤‘ìš”ì„±ì„ ë°°ì›ë‹ˆë‹¤.

ğŸš€ ì‹¤ì œ ì‘ìš© ê°€ëŠ¥í•œ ì˜ˆì œë“¤ì„ í†µí•´ ë¬¸ì œ í•´ê²° ì—­ëŸ‰ê³¼ ì‘ìš©ë ¥ì„ ê¸°ë¦…ë‹ˆë‹¤.
'''
import numpy as np
import tensorflow as tf
import random

# ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
tf.random.set_seed(777)
random.seed(777)

mnist = tf.keras.datasets.mnist

# 1ï¸âƒ£ [ë°ì´í„° ë¡œë“œ ë° ì •ê·œí™”]
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_test = x_test / 255.0
x_train = x_train / 255.0

# â­ [í•µì‹¬ ì°¨ì´ì ] 2ì°¨ì› í˜•íƒœ ìœ ì§€!
# ì´ì „: (60000, 784)ë¡œ í‰íƒ„í™”
# CNN: (60000, 28, 28, 1)ë¡œ ë³€í™˜. (ë°ì´í„°ìˆ˜, ê°€ë¡œ, ì„¸ë¡œ, ì±„ë„ìˆ˜)
# í‘ë°± ì´ë¯¸ì§€ì´ë¯€ë¡œ ì±„ë„(ìƒ‰ìƒ)ì€ 1ì…ë‹ˆë‹¤. (ì»¬ëŸ¬ë©´ 3)
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)

# ì›-í•« ì¸ì½”ë”©
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
learning_rate = 0.001
training_epochs = 12
batch_size = 128

tf.model = tf.keras.Sequential()

# 2ï¸âƒ£ [Layer 1: íŠ¹ì„± ì¶”ì¶œê¸°]
# Conv2D: 3x3 í¬ê¸°ì˜ ë‹ë³´ê¸°(í•„í„°) 16ê°œë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ í›‘ìœ¼ë©° íŠ¹ì§•ì„ ì°¾ìŠµë‹ˆë‹¤.
tf.model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))
# MaxPooling2D: 2x2 í¬ê¸°ë¡œ ë¬¶ì–´ì„œ ê°€ì¥ í° ê°’ë§Œ ë‚¨ê²¨ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
tf.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))

# 3ï¸âƒ£ [Layer 2: ë” ê¹Šì€ íŠ¹ì„± ì¶”ì¶œê¸°]
# ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì‹œ 3x3 í•„í„° 32ê°œë¥¼ ì‚¬ìš©í•´ ë” ë³µì¡í•œ íŠ¹ì§•(ë™ê·¸ë¼ë¯¸, êº¾ì¸ ì„  ë“±)ì„ ì°¾ìŠµë‹ˆë‹¤.
tf.model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))
tf.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))

# 4ï¸âƒ£ [Layer 3: ë¶„ë¥˜ê¸° (Fully Connected)]
# Flatten: ì¶”ì¶œì´ ëë‚œ íŠ¹ì§•ë“¤ì„ ìµœì¢… íŒë‹¨í•˜ê¸° ìœ„í•´ 1ì°¨ì›ìœ¼ë¡œ ì«™ í…ë‹ˆë‹¤.
tf.model.add(tf.keras.layers.Flatten())
# Dense: ìµœì¢…ì ìœ¼ë¡œ ì–´ë–¤ ìˆ«ìì¸ì§€ 10ê°œë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. (ì¶œë ¥ì¸µ)
tf.model.add(tf.keras.layers.Dense(units=10, kernel_initializer='glorot_normal', activation='softmax'))

# 5ï¸âƒ£ [ì»´íŒŒì¼ ë° í•™ìŠµ]
tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])
tf.model.summary()

tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)

# 6ï¸âƒ£ [ì˜ˆì¸¡ ë° í‰ê°€]
y_predicted = tf.model.predict(x_test)
for x in range(0, 10):
    random_index = random.randint(0, x_test.shape[0]-1)
    print("index: ", random_index,
          "actual y: ", np.argmax(y_test[random_index]),
          "predicted y: ", np.argmax(y_predicted[random_index]))

evaluation = tf.model.evaluate(x_test, y_test)
print('loss: ', evaluation[0])
print('accuracy', evaluation[1])
#==================================================================
# CNN(í•©ì„±ê³± ì‹ ê²½ë§)
# ê¸°ì¡´ì˜ ì¼ë°˜ ì‹ ê²½ë§ì´ 2ì°¨ì› ì´ë¯¸ì§€ë¥¼ 1ì°¨ì›ìœ¼ë¡œ í´ ë²„ë¦°ê²ƒê³¼ ë‹¬ë¦¬, 
# CNNì€ í•©ì„±ê³±ê³¼ í’€ë§ì„ í†µí•´ì„œ ê³µê°„ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì‚´ë¦°ë‹¤.

# ì´ì „ ì˜ˆì œì—ì„œëŠ” íŒŒë¼ë¯¸í„°ê°€ 100ë§Œê°œì—ì„œ 230ë§Œê°œì˜€ì§€ë§Œ, CNNëª¨ë¸ì—ì„œëŠ” 12810ê°œì´ë‹¤.
# ìµœì¢… ì‹¤ì „ í…ŒìŠ¤íŠ¸ì˜ ì •í™•ë„ëŠ” 98.76%ì´ê³  í•™ìŠµ ì •í™•ë„ëŠ” 99.17%ë¡œ í•™ìŠµ ì†ë„ì™€ ì•ˆì •ì„± ëª¨ë‘ ë” ì»¤ì¡Œë‹¤.

# CNNì„ í†µí•´ì„œ ê³µê°„ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©° íŠ¹ì§•ë§Œ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê²ƒì´
# ì—°ì‚°ëŸ‰ê³¼ ì •í™•ë„ ë©´ì—ì„œ ë” ìš°ìˆ˜í•˜ë‹¤.

