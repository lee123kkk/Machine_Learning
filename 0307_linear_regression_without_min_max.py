# tf2-07-2-linear_regression_without_min_max.py

import tensorflow as tf
import numpy as np

# 1. [ë°ì´í„° ì¤€ë¹„]
# ì›ë³¸ ë°ì´í„°: ê±°ë˜ëŸ‰(3ë²ˆì§¸ ì—´)ì´ ë‹¤ë¥¸ ê°’ë“¤ì— ë¹„í•´ 1000ë°° ì´ìƒ í½ë‹ˆë‹¤.
xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],
               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],
               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],
               [816, 820.958984, 1008100, 815.48999, 819.23999],
               [819.359985, 823, 1188100, 818.469971, 818.97998],
               [819, 823, 1198100, 816, 820.450012],
               [811.700012, 815.25, 1098100, 809.780029, 813.669983],
               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])

# ========================================================
# [í•µì‹¬ ìˆ˜ì •] ì •ê·œí™” í•¨ìˆ˜ (Min-Max Scaling)
# ê³µì‹: (X - ìµœì†Ÿê°’) / (ìµœëŒ“ê°’ - ìµœì†Ÿê°’)
# ëª¨ë“  ë°ì´í„°ë¥¼ 0 ~ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
# ========================================================
def min_max_scaler(data):
    numerator = data - np.min(data, 0) # ë¶„ì: ë°ì´í„° - ìµœì†Ÿê°’
    denominator = np.max(data, 0) - np.min(data, 0) # ë¶„ëª¨: ìµœëŒ“ê°’ - ìµœì†Ÿê°’
    # ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•„ì£¼ ì‘ì€ ìˆ˜(1e-7)ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.
    return numerator / (denominator + 1e-7)

# ë°ì´í„°ì— ì •ê·œí™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤!
# ì´ì œ 100ë§Œì´ì—ˆë˜ ê±°ë˜ëŸ‰ë„ 0.x ë‹¨ìœ„ë¡œ ì–Œì „í•´ì§‘ë‹ˆë‹¤.
xy = min_max_scaler(xy)
print("ğŸ” ì •ê·œí™”ëœ ë°ì´í„°(ì¼ë¶€):\n", xy[0]) 

# 2. [ë°ì´í„° ë¶„ë¦¬]
x_data = xy[:, 0:-1] # ì…ë ¥: ì‹œê°€, ê³ ê°€, ê±°ë˜ëŸ‰, ì €ê°€
y_data = xy[:, [-1]] # ì¶œë ¥: ì¢…ê°€

# 3. [ëª¨ë¸ êµ¬ì„±]
tf.model = tf.keras.Sequential()
tf.model.add(tf.keras.layers.Dense(units=1, input_dim=4))
tf.model.add(tf.keras.layers.Activation('linear'))

# 4. [ì»´íŒŒì¼]
# ì •ê·œí™”ë¥¼ í–ˆê¸° ë•Œë¬¸ì— í•™ìŠµë¥ (learning_rate)ì„ 1e-5ì²˜ëŸ¼ ë„ˆë¬´ ì‘ê²Œ ì¡ì„ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
# ì¼ë°˜ì ì¸ 0.01ì´ë‚˜ 0.1ì„ ì¨ë„ ì˜ í•™ìŠµë©ë‹ˆë‹¤.
tf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.1))
tf.model.summary()

# 5. [í•™ìŠµ ìˆ˜í–‰]
print("\nğŸ“‰ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤ (Lossê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ í™•ì¸í•˜ì„¸ìš”!)")
history = tf.model.fit(x_data, y_data, epochs=1000, verbose=0)

# í•™ìŠµ ê³¼ì • í™•ì¸ (ë§ˆì§€ë§‰ 5ë²ˆë§Œ ì¶œë ¥)
print("Final Losses:", history.history['loss'][-5:])

# 6. [ê²°ê³¼ ì˜ˆì¸¡]
# ì£¼ì˜: ì •ê·œí™”ëœ ë°ì´í„°ë¡œ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ, ì˜ˆì¸¡ ê²°ê³¼ë„ 0~1 ì‚¬ì´ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
# ì‹¤ì œ ì£¼ì‹ ê°€ê²©ì„ ë³´ë ¤ë©´ ì—­ë³€í™˜(Inverse Transform) ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
predict = tf.model.predict(x_data)
print("\n[ì˜ˆì¸¡ê°’ (0~1 ìŠ¤ì¼€ì¼)] vs [ì‹¤ì œê°’ (0~1 ìŠ¤ì¼€ì¼)]")
print(np.hstack((predict, y_data))[0:5]) # 5ê°œë§Œ ë¹„êµ ì¶œë ¥

#===================================================================
# ì›ë³¸: ë°ì´í„° ê°„ì˜ ìŠ¤ì¼€ì¼ì˜ ì°¨ì´ ë•Œë¬¸ì— ë°œì‚°í•˜ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤.
# ëª¨ë“  ë°ì´í„°ë¥¼ 0ê³¼ 1ì‚¬ì´ì˜ ì‘ì€ ìˆ«ìë¡œ ì •ê·œí™”í•œë‹¤.(XY Data Scaling)

# ì…ë ¥ ë°ì´í„° ê°„ì˜ ìˆ«ì ë‹¨ìœ„ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ í•™ìŠµì´ ë°œì‚°í•˜ë¯€ë¡œ,
# 0ê³¼ 1ì‚¬ì´ë¡œ ë§ì¶°ì£¼ëŠ” Min-Max ì •ê·œí™” ê³¼ì •ì´ í•„ìˆ˜ì ì´ë‹¤.

