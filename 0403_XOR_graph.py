import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
tf.random.set_seed(777)

# 1. [ë°ì´í„° ì¤€ë¹„] XOR
x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)

# 2. [ëª¨ë¸ êµ¬ì„±] (ì„±ê³µí•˜ëŠ” Adam ëª¨ë¸ ì‚¬ìš©)
input_layer = tf.keras.Input(shape=(2,))
# ì€ë‹‰ì¸µ (Hidden Layer): 10ê°œì˜ ë‰´ëŸ°
hidden_layer = tf.keras.layers.Dense(units=10, activation='sigmoid')(input_layer)
# ì¶œë ¥ì¸µ (Output Layer): 1ê°œì˜ ë‰´ëŸ°
output_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(hidden_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)

# 3. [ì»´íŒŒì¼] Optimizerë¥¼ Adamìœ¼ë¡œ ì„¤ì • (í•™ìŠµ ì„±ê³µì˜ ì—´ì‡ !)
model.compile(loss='binary_crossentropy', 
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), 
              metrics=['accuracy'])

# 4. [í•™ìŠµ ìˆ˜í–‰]
print("ğŸ§  XOR íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
history = model.fit(x_data, y_data, epochs=3000, verbose=0)
print("âœ… í•™ìŠµ ì™„ë£Œ!")

# ==========================================================
# 5. [ì‹œê°í™”] 4ë¶„í•  ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
# ==========================================================
fig = plt.figure(figsize=(14, 10))

# --- (1) ì¢Œì¸¡ ìƒë‹¨: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ëª¨ì–‘ ---
ax1 = fig.add_subplot(2, 2, 1)
z = np.linspace(-10, 10, 100)
sigmoid = 1 / (1 + np.exp(-z))
ax1.plot(z, sigmoid, 'b-')
ax1.set_title("Sigmoid Activation Function")
ax1.set_xlabel("Input (z)")
ax1.set_ylabel("Output (0~1)")
ax1.grid(True)

# --- (2) ìš°ì¸¡ ìƒë‹¨: í•™ìŠµ Loss ê·¸ë˜í”„ ---
ax2 = fig.add_subplot(2, 2, 2)
ax2.plot(history.history['loss'], 'b-')
ax2.set_title("Training Loss (Binary Crossentropy)")
ax2.set_xlabel("Epoch")
ax2.set_ylabel("Loss")
ax2.grid(True)

# --- (3) ì¢Œì¸¡ í•˜ë‹¨: ìµœì¢… ì¶œë ¥(Output)ì˜ 3D ì§€í˜• ---
# ë°”ë‘‘íŒ ì¢Œí‘œ ë§Œë“¤ê¸° (ì…ë ¥ ê³µê°„ ì „ì²´ë¥¼ í›‘ì–´ë³´ê¸° ìœ„í•¨)
xx, yy = np.meshgrid(np.arange(-0.5, 1.5, 0.1), np.arange(-0.5, 1.5, 0.1))
grid_points = np.c_[xx.ravel(), yy.ravel()]

# ëª¨ë¸ ì „ì²´ë¥¼ í†µê³¼í•œ ìµœì¢… ì˜ˆì¸¡ê°’
final_preds = model.predict(grid_points, verbose=0)
Z_final = final_preds.reshape(xx.shape)

ax3 = fig.add_subplot(2, 2, 3, projection='3d')
# XOR ë¬¸ì œê°€ í’€ë ¸ë‹¤ë©´ (0,1), (1,0) ë¶€ë¶„ë§Œ ì†Ÿì•„ì˜¤ë¥¸ ëª¨ì–‘ì´ ë©ë‹ˆë‹¤.
ax3.plot_surface(xx, yy, Z_final, cmap='Blues', alpha=0.8, edgecolor='none')
ax3.set_title("3D Output Surface (Final Decision)")
ax3.set_xlabel("x1")
ax3.set_ylabel("x2")
ax3.set_zlabel("Probability")

# --- (4) ìš°ì¸¡ í•˜ë‹¨: ì€ë‹‰ì¸µ ì²« ë²ˆì§¸ ë‰´ëŸ°ì˜ 3D ì§€í˜• ---
# [í•µì‹¬] ëª¨ë¸ì˜ ì¤‘ê°„(ì€ë‹‰ì¸µ) ê²°ê³¼ë§Œ ë½‘ì•„ë‚´ëŠ” ë¶€ë¶„ ëª¨ë¸ ë§Œë“¤ê¸°
hidden_layer_model = tf.keras.Model(inputs=model.input, outputs=hidden_layer)
hidden_preds = hidden_layer_model.predict(grid_points, verbose=0)

# ì€ë‹‰ì¸µì—ëŠ” 10ê°œì˜ ë‰´ëŸ°ì´ ìˆëŠ”ë°, ê·¸ ì¤‘ ì²« ë²ˆì§¸(0ë²ˆ) ë‰´ëŸ°ì˜ ìƒê°ë§Œ ì—¿ë´…ë‹ˆë‹¤.
Z_hidden = hidden_preds[:, 0].reshape(xx.shape)

ax4 = fig.add_subplot(2, 2, 4, projection='3d')
# ë‹¨ì¼ ë‰´ëŸ°ì€ ì§ì„ (í‰ë©´) í•˜ë‚˜ë¡œ ì„¸ìƒì„ ë‚˜ëˆ„ë¯€ë¡œ 'ì ˆë²½' ê°™ì€ ëª¨ì–‘ì´ ë‚˜ì˜µë‹ˆë‹¤.
ax4.plot_surface(xx, yy, Z_hidden, cmap='viridis', alpha=0.8, edgecolor='none')
ax4.set_title("Hidden Neuron #1 Activation Surface")
ax4.set_xlabel("x1")
ax4.set_ylabel("x2")
ax4.set_zlabel("Activation")

plt.tight_layout()
plt.show()
#=====================================================================
# sigmoid í˜•íƒœ, lossë³€í™”, ìµœì¢… ì¶œë ¥ì˜ 3D, ì€ë‹‰ì¸µ ë‰´ëŸ°ì˜ 3D
# sigmoid: ìˆ«ìë¥¼ 0ê³¼ 1 ì‚¬ì´ë¡œ ì••ì¶•
# ìš°ìƒë‹¨(loss): lossê°€ ë°”ë‹¥ìœ¼ë¡œ ë–¨ì–´ì§. í•™ìŠµì´ ì™„ë²½í•˜ê²Œ ë˜ì—ˆë‹¤.
# ì¢Œí•˜ë‹¨(ìµœì¢… ê²°ê³¼): XORì˜ í•´ë‹µ
# ìš°í•˜ë‹¨(ì€ë‹‰ì¸µ ë‰´ëŸ°): ë¹„ìŠ¤ë“¬í•œ ì ˆë²½ ëª¨ì–‘ (1ê°œë§Œ ì‚¬ìš©í•´ì„œëŠ” XORë¬¸ì œ í•´ê²° ë¶ˆê°€ëŠ¥)

# í•˜ë‚˜ì˜ ë‰´ëŸ°ì€ ì„ í˜• ë¶„ë¦¬ë°–ì— ëª» ë§Œë“¤ì§€ë§Œ, ì—¬ëŸ¬ê°œë¥¼ ëª¨ì•„ì„œ ì€ë‹‰ì¸µì„ ë§Œë“¤ë©´ ë¹„ì„ í˜• ë¬¸ì œë„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.
