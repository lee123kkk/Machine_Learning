# 0700_CIFAR10_DeepCNN_Augmentation

import tensorflow as tf
import os
import datetime

# 1ï¸âƒ£ [ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬] TFDS ëŒ€ì‹  ê¸°ë³¸ Keras ë°ì´í„°ì…‹ ì‚¬ìš© (ë¹ ë¥¸ ë¡œë“œë¥¼ ìœ„í•´)
print("ğŸš€ CIFAR-10 ë°ì´í„° ë¡œë“œ ì¤‘...")
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# 0~1 ì •ê·œí™” ë° float32 ë³€í™˜ (GPU ì—°ì‚° ìµœì í™”)
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# ì›-í•« ì¸ì½”ë”©
nb_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, nb_classes)
y_test = tf.keras.utils.to_categorical(y_test, nb_classes)

# 2ï¸âƒ£ [ë°ì´í„° ì¦ê°• (Data Augmentation)]
# Keras ë ˆì´ì–´ë¡œ ë§Œë“¤ì–´ ëª¨ë¸ ë‚´ë¶€ì— ì‚½ì…í•˜ë©´ GPUë¥¼ ì‚¬ìš©í•´ ë§¤ìš° ë¹ ë¥´ê²Œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
data_augmentation = tf.keras.Sequential([
    # ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¢Œìš° ë°˜ì „
    tf.keras.layers.RandomFlip("horizontal", input_shape=(32, 32, 3)),
    # ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ 10% ë¬´ì‘ìœ„ íšŒì „
    tf.keras.layers.RandomRotation(0.1),
    # ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ í™•ëŒ€/ì¶•ì†Œ
    tf.keras.layers.RandomZoom(0.1),
], name="data_augmentation")

# 3ï¸âƒ£ [ëª¨ë¸ êµ¬ì„±] ë” ê¹Šì€ CNN (VGG ìŠ¤íƒ€ì¼: Conv-Conv-Pool êµ¬ì¡°)
model = tf.keras.Sequential()

# ì¦ê°• ë ˆì´ì–´ë¥¼ ëª¨ë¸ì˜ ë§¨ ì²˜ìŒì— ë°°ì¹˜
model.add(data_augmentation)

# [Block 1] ì–•ì€ íŠ¹ì§•(ì„ , ìƒ‰ê° ë“±) ì¶”ì¶œ
model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.2)) # ë‚®ì€ ì¸µì€ ì¡°ê¸ˆë§Œ ë“œë¡­ì•„ì›ƒ

# [Block 2] ì¤‘ê°„ íŠ¹ì§•(í˜•íƒœ, ì§ˆê° ë“±) ì¶”ì¶œ (ì±„ë„ì„ 64ê°œë¡œ ëŠ˜ë¦¼)
model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.3))

# [Block 3] ê¹Šì€ íŠ¹ì§•(ê³ ì°¨ì› íŒ¨í„´) ì¶”ì¶œ (ì±„ë„ì„ 128ê°œë¡œ ëŠ˜ë¦¼)
model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.4))

# [ë¶„ë¥˜ê¸° (Classifier)]
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(0.5)) # ê¹Šì€ ì¸µì€ ë§ì´ ë“œë¡­ì•„ì›ƒ
model.add(tf.keras.layers.Dense(nb_classes, activation='softmax'))

# 4ï¸âƒ£ [ì»´íŒŒì¼]
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])
model.summary()

# 5ï¸âƒ£ [â­í•µì‹¬: ì½œë°± í•¨ìˆ˜ ì„¤ì •]
# (1) TensorBoard: í•™ìŠµ ê³¼ì •ì„ ì‹œê°í™”í•˜ì—¬ ì €ì¥
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

# (2) ModelCheckpoint: í•™ìŠµ ì¤‘ ê²€ì¦ ì •í™•ë„(val_accuracy)ê°€ ê°€ì¥ ë†’ì•˜ë˜ ìµœê³  ëª¨ë¸ë§Œ ì €ì¥
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    filepath='best_cifar10_cnn.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

# (3) EarlyStopping: ì„±ëŠ¥ì´ ë” ì´ìƒ ì•ˆ ì˜¤ë¥´ë©´ 10ë¶„ ë‚´ì— ëë‚´ê¸° ìœ„í•´ ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
early_stopping_cb = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy', 
    patience=8, # 8ë²ˆ ì—°ì†ìœ¼ë¡œ ì„±ëŠ¥ì´ ì•ˆ ì˜¤ë¥´ë©´ í•™ìŠµ ì¤‘ë‹¨
    restore_best_weights=True # ê°€ì¥ ì¢‹ì•˜ë˜ ê°€ì¤‘ì¹˜ë¡œ ë³µêµ¬
)

# 6ï¸âƒ£ [í•™ìŠµ ìˆ˜í–‰] RTX 4060ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸!
training_epochs = 40 # 40ë²ˆ ëŒë ¤ë„ 4060ì´ë©´ ê¸ˆë°© ëë‚©ë‹ˆë‹¤.
batch_size = 128

print("\nğŸš€ ë°ì´í„° ì¦ê°• ë° VGG ìŠ¤íƒ€ì¼ CNN í•™ìŠµ ì‹œì‘...")
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=training_epochs,
                    validation_data=(x_test, y_test),
                    callbacks=[tensorboard_cb, checkpoint_cb, early_stopping_cb])

# 7ï¸âƒ£ [ìµœì¢… í‰ê°€]
print("\n" + "="*50)
evaluation = model.evaluate(x_test, y_test)
print(f"ìµœì¢… Loss: {evaluation[0]:.4f}")
print(f"ìµœì¢… ì‹¤ì „ ì •í™•ë„(Accuracy): {evaluation[1]*100:.2f}%")

# ë°ì´í„° ì¦ê°• + ë” ê¹Šì€ CNN ì„¤ê³„

# í•™ìŠµ ì´ˆê¸°ì˜ ì •í™•ë„ í•˜ë½ í˜„ìƒ: 
# ë°ì´í„° ì¦ê°• ë ˆì´ì–´ ë•Œë¬¸ì— í›ˆë ¨ ë°ì´í„°ê°€ ë’¤í‹€ë¦¬ê³  ìˆê¸° ë•Œë¬¸ì— ì´ˆê¸°ì—ëŠ” ì´ì „ ëª¨ë¸ë³´ë‹¤ ë‚®ì•„ì¡Œë‹¤.
# ê¾¸ì¤€í•œ ìš°ìƒí–¥ ê³¡ì„ :
# ì‹¤ì „ ì •í™•ë„ê°€ ê¾¸ì¤€íˆ ì˜¤ë¥´ëŠ” ëª¨ìŠµì„ ë³´ì´ë©° ê³¼ì í•©ì„ ì–µì œí•˜ê³  ìˆë‹¤.

# ì´ˆë°˜ ì •í™•ë„ëŠ” 29.6%ë¡œ ë‚®ê²Œ ì¶œë°œí–ˆë‹¤.
# íŒŒë€ì„ ê³¼ ì£¼í™©ì„ ì´ í•¨ê¼ ìš°ìƒí–¥í•˜ê³  ìˆë‹¤. ê³¼ì í•©ì„ ì˜ ë°©ì§€í•˜ê³  ìˆë‹¤.
# 8ë²ˆ ì—°ì†ìœ¼ë¡œ ì„±ëŠ¥ì´ ì˜¤ë¥´ì§€ ì•Šì ì¡°ê¸° ì¢…ë£Œë˜ì—ˆë‹¤. ì´í¬í¬ 26ì—ì„œ í•™ìŠµì„ ë©ˆì·„ê³ , 
# ìµœì¢… ì •í™•ë„ 76.33%ë¥¼ ê¸°ë¡í–ˆë‹¤.

