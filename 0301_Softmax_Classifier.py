# Lab 6 Softmax Classifier

import tensorflow as tf
import numpy as np

# 1. ë°ì´í„° ì¤€ë¹„
x_raw = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5],
         [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]
y_raw = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],
         [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]

x_data = np.array(x_raw, dtype=np.float32)
y_data = np.array(y_raw, dtype=np.float32)
nb_classes = 3

# 2. ëª¨ë¸ êµ¬ì„±
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(units=nb_classes, input_dim=4, activation='softmax'))
model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),
              metrics=['accuracy'])

# ==========================================================
# [í•µì‹¬] 100ë²ˆë§ˆë‹¤ ìƒì¡´ ì‹ ê³ ë¥¼ í•˜ëŠ” ê°ì‹œì(Callback) í´ë˜ìŠ¤
# ==========================================================
class MyPrinter(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % 100 == 0:
            print(f"Epoch {epoch + 1:4d}/2000 | Loss: {logs['loss']:.6f} | Acc: {logs['accuracy']:.2f}")
# ==========================================================

print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤! (100ë²ˆë§ˆë‹¤ ë¡œê·¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤)")

# 3. ëª¨ë¸ í•™ìŠµ (verbose=0ìœ¼ë¡œ ê¸°ë³¸ ì¶œë ¥ì€ ë„ê³ , ìš°ë¦¬ê°€ ë§Œë“  ê°ì‹œìë¥¼ íˆ¬ì…!)
history = model.fit(x_data, y_data, epochs=2000, 
                    verbose=0, 
                    callbacks=[MyPrinter()])

print("âœ… í•™ìŠµ ì™„ë£Œ!\n")

# 4. ì „ì²´ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ (ì›ë˜ ì½”ë“œì— ìˆë˜ ëª¨ë“  í…ŒìŠ¤íŠ¸ ë¶€í™œ)
print('-------------- [Test A] --------------')
a = model.predict(np.array([[1, 11, 7, 9]], dtype=np.float32))
print(f"ì˜ˆì¸¡ í™•ë¥ : {a}")
print(f"ì„ íƒëœ í´ë˜ìŠ¤: {np.argmax(a, axis=1)}")

print('-------------- [Test B] --------------')
b = model.predict(np.array([[1, 3, 4, 3]], dtype=np.float32))
print(f"ì˜ˆì¸¡ í™•ë¥ : {b}")
print(f"ì„ íƒëœ í´ë˜ìŠ¤: {np.argmax(b, axis=1)}")

print('-------------- [Test C] --------------')
c = model.predict(np.array([[1, 1, 0, 1]], dtype=np.float32))
c_onehot = np.argmax(c, axis=-1)
print(f"ì˜ˆì¸¡ í™•ë¥ : {c}")
print(f"ì„ íƒëœ í´ë˜ìŠ¤: {c_onehot}")

print('-------------- [Test All] --------------')
all_data = np.array([[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]], dtype=np.float32)
all_predict = model.predict(all_data)
all_onehot = np.argmax(all_predict, axis=1)
print(f"ì „ì²´ ì˜ˆì¸¡ í™•ë¥ :\n{all_predict}")
print(f"ì „ì²´ ì„ íƒ ê²°ê³¼: {all_onehot}")

#============================================================
# 3ê°œ ì´ìƒì˜ ì„ íƒì§€ ì¤‘ í•˜ë‚˜ë¥¼ ê³ ë¥´ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ì˜ˆì œ

# ì›-í•« ì¸ì½”ë”©:ì •ë‹µì´ ìˆ«ì í•˜ë‚˜ê°€ ì•„ë‹ˆë¼ [0,0,1]ì²˜ëŸ¼ ë˜ì–´ ìˆë‹¤. ì´ ê²½ìš°ì— 3ë²ˆì§¸ë¡œ ë¶„ë¥˜í•œë‹¤.
# ì†Œí”„íŠ¸ë§¥ìŠ¤: ëª¨ë¸ì˜ ì¶œë ¥ê°’ì„ ëª¨ë‘ ë”í•˜ë©´ 1.0ì´ ë˜ë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤.
# ì•„ê·¸ë§¥ìŠ¤: í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê³³ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì•„ë‚¸ë‹¤.

# softmaxí•¨ìˆ˜ë¥¼ í†µí•´ ì¶œë ¥ê°’ì„ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ê³  ì›-í•« ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ 
# ì„¸ ê°€ì§€ ì´ìƒì˜ ì„ íƒì§€ê°€ ìˆëŠ” ë³µì¡í•œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.
